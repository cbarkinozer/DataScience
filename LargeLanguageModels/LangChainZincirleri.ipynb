{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Zincir bir LLM'in temel yapıtaşıdır . Zincirler bir LLM'ler ile promptların birleştirilmiş halidirler. Sonra bu yapıtaşı bloklar bir araya getirilerek daha büyük bir seri işlemler gerçekleştirilebilir."
      ],
      "metadata": {
        "id": "Rvax2fEJBPp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # lokal .env dosyasını oku\n",
        "\n",
        "#!pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('Data.csv') # Product Review şeklinde 2 sütunu olan bir data: https://s172-31-14-227p20192.lab-aws-production.deeplearning.ai/edit/Data.csv#\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nsrNzquwBSjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMChain\n",
        "LLMChain, LLM ve promptu birleştirip zinciri ortaya çıkaran sınıftır."
      ],
      "metadata": {
        "id": "IyEqeDwDBUAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "DQgO-M6mBUtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9) # 1'e yakınken daha yaratıcı ve hata ihtimali yüksek sonuçlar verir"
      ],
      "metadata": {
        "id": "o11jRxj4BV77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"{product} üreten bir şirketi en iyi tanımlayan isim nedir?\"\n",
        ")"
      ],
      "metadata": {
        "id": "nVPE8LqBBXZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt) # chain = llm + prompt"
      ],
      "metadata": {
        "id": "m5AR7cUMBZJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product = \"jumbo Çarşaf Takımı\"\n",
        "chain.run(product) # Prompt'u LLM'e iletir"
      ],
      "metadata": {
        "id": "dUWb4R8aBaep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimpleSequentialChain (Basit Sıralı Zincir)\n",
        "SimpleSequentialChain'de bir zincirin çıktısının bir sonraki zincirin girdisi olduğu birden fazla zincir birleştirilir. İki tür sıralı zincir vardır: Tek girdi/çıktısı olan SimpleSequentialChain ve çoklu girdi/çıktısı olan SequentialChain ."
      ],
      "metadata": {
        "id": "GSi-6zRkBc9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain"
      ],
      "metadata": {
        "id": "0kBqupTABecm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9)\n",
        "\n",
        "# prompt şablonu 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"{product} üreten bir şirketi en iyi tanımlayan isim nedir?\"\n",
        ")\n",
        "\n",
        "# 1. Zincir\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
      ],
      "metadata": {
        "id": "y1o4PuRNBkCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt şablonu 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Şu şirket için 20 kelimelik bir açıklama yazın:{company_name}\"\n",
        ")\n",
        "# 2. Zincir\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ],
      "metadata": {
        "id": "6jUIOUZoBmO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
        "                                             verbose=True\n",
        "                                            )\n",
        "overall_simple_chain.run(product)"
      ],
      "metadata": {
        "id": "8AO_0f4iBoG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SequentialChain (Sıralı Zincir)"
      ],
      "metadata": {
        "id": "W2hqnOyyBqpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain"
      ],
      "metadata": {
        "id": "-ZoFyx1yBr0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9)\n",
        "\n",
        "# 1. Prompt Şablonu: ingilizceye çevir\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Aşağıdaki yorumu Türkçeye çevir:\"\n",
        "     \"\\n\\n{Review}\"\n",
        ")\n",
        "# 1. Zincir: girdi= İnceleme ve çıktı= Turkish_Review\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
        "                     output_key=\"Turkish_Review\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "N1jCEN7UBtYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Aşağıdaki incelemeyi 1 cümle ile özetler misiniz:\"\n",
        "     \"\\n\\n{English_Review}\"\n",
        ")\n",
        "# 2. zincir: girdi= English_Review ve çıktı= summary\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key=\"summary\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "KB9Nwn9BBvGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. prompt şablonu: Türkçeye çevir\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Aşağıdaki inceleme hangi dildedir:\\n\\n{Review}\"\n",
        ")\n",
        "# 3. zincir: girdi= İnceleme ve çıktı= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"language\"\n",
        "                      )"
      ],
      "metadata": {
        "id": "TznZUoteBw2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. prompt şablonu: followup_message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Aşağıdaki özete belirtilen dilde devam eden \"\n",
        "    \"bir yanıt yazın:\"\n",
        "    \"\\in\\Özet: {summary}\\in\\Dil: {language}\"\n",
        ")\n",
        "# 4. zincir: girdi= özet, dil ve çıktı= followup_message\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                      output_key=\"followup_message\"\n",
        "                     )"
      ],
      "metadata": {
        "id": "yQv_xFELBytG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_chain: girdi= Review\n",
        "# ve çıktı= English_Review,summary, followup_message\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "u1V4bINrB0Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = df.Review[5]\n",
        "overall_chain(review)"
      ],
      "metadata": {
        "id": "5fi388GcB2e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Router Chain (Yönlendirici Zinciri)"
      ],
      "metadata": {
        "id": "E6ciMu2AB5og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"Sen çok zeki bir fizik profesörüsün. \\\n",
        "Fizikle ilgili soruları özlü ve anlaşılması kolay bir şekilde yanıtlamakta harikasın. \\\n",
        "Bir sorunun cevabını bilmiyorsan, bilmediğini kabul edersin. \\\n",
        "\n",
        "İşte sana bir soru:\n",
        "{input}\"\"\"\n",
        "\n",
        "math_template = \"\"\"Sen çok iyi bir matematikçisin. \\\n",
        "Matematik sorularını cevaplamakta harikasın. \\\n",
        "Siz çok iyisiniz çünkü zor problemleri bileşen parçalarına ayırabilir, \\\n",
        "bileşen parçalarına cevap verebilir ve daha sonra daha geniş soruyu yanıtlamak \\\n",
        "için bunları bir araya getirebilirsiniz.\n",
        "\n",
        "İşte sana bir soru:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"Sen çok iyi bir tarihçisin. \\\n",
        "Çeşitli tarihsel dönemlerden insanlar, olaylar ve bağlamlar hakkında \\\n",
        "mükemmel bir bilgiye ve anlayışa sahipsiniz. \\\n",
        "Geçmişi düşünme, yansıtma, tartışma, tartışma ve değerlendirme yeteneğine sahipsiniz. \\\n",
        "Tarihsel kanıtlara ve açıklamalarınızı ve yargılarınızı desteklemek için \\\n",
        "bunları kullanma yeteneğine saygı duyuyorsunuz.\n",
        "\n",
        "İşte sana bir soru:\n",
        "{input}\"\"\"\n",
        "\n",
        "computerscience_template = \"\"\" Başarılı bir bilgisayar bilimcisisiniz. \\\n",
        "Yaratıcılığa, işbirliğine, ileri görüşlülüğe, kendine güvene, \\\n",
        "güçlü problem çözme yeteneklerine, teorileri ve algoritmaları anlamaya ve\\\n",
        " mükemmel iletişim becerilerine tutkunuz var. \\\n",
        "Kodlama sorularını yanıtlamakta harikasınız. \\\n",
        "Çok iyisin çünkü bir makinenin kolayca yorumlayabileceği zorunlu adımlarla \\\n",
        "bir sorunu nasıl çözeceğini biliyorsun ve zaman karmaşıklığı ile \\\n",
        "mekan karmaşıklığı arasında iyi bir dengeye sahip bir çözümü nasıl \\\n",
        "seçeceğini biliyorsun.\n",
        "\n",
        "İşte sana bir soru:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "edUyurvxB6m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"fizik\",\n",
        "        \"description\": \"Fizik sorularını yanıtlamak için iyi\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"matematik\",\n",
        "        \"description\": \"Matematik sorularını yanıtlamak için iyi\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"tarih\",\n",
        "        \"description\": \"Tarih sorularını yanıtlamak için iyi\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"bilgisayar bilimi\",\n",
        "        \"description\": \"Bilgisayar bilimi sorularını yanıtlamak için iyi\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "lc0pzODaCBqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MultiPromptChain (çoklu prompt zinciri**) birden fazla prompt şablonu arasında yönlendirme yaparken kullanılır.\n",
        "\n",
        "**LLMRouterChain (LLM yönelndirici zinciri)**, farklı alt zincirler arasında yönlendirme yapmak için bir dil modeli kullanır. Yukarıda verilen açıklama ve adın kullanıldığı yer burasıdır.\n",
        "\n",
        "**RouterOutputParser (yönlendirici çıktı ayrıştırıcı)**, LLM çıktısını hangi zincirin kullanılması gerektiğini ve bu zincire hangi girdinin kullanılması gerektiğini belirlemek için kullanılabilecek bir sözlüğe ayrıştırır.\n",
        "\n",
        "**destination_chains**: Yönlendirici zinciri tarafından çağırılan zincirlerdir. Her destination zinciri bir LLM zinciridir.\n",
        "\n",
        "**default_chain**: Bu zincir, yönlendiricinin hangi zinciri kullanacağına karar veremediğinde kullanılır."
      ],
      "metadata": {
        "id": "m2JfQFtuCHJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "ALDXAXLZCS5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "Vqy76IpKCVm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ],
      "metadata": {
        "id": "WykBo0xRCXB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "metadata": {
        "id": "u4cf4UOFCYOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Bir dil modeline ham metin girişi \\\n",
        "verildiğinde, girdi için en uygun model bilgi istemini seçin. \\\n",
        "Size mevcut istemlerin adları ve istemin en uygun olduğu şeyin açıklaması \\\n",
        "verilecektir. Ayrıca, gözden geçirmenin nihayetinde dil modelinden daha \\\n",
        "iyi bir yanıta yol açacağını düşünüyorsanız, orijinal girdiyi de \\\n",
        "gözden geçirebilirsiniz.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Aşağıdaki gibi biçimlendirilmiş bir JSON nesnesine sahip bir işaretleme kod pasajı döndürün:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ kullanılacak bilgi isteminin adı veya \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ orijinal girdinin potansiyel olarak değiştirilmiş bir versiyonu\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\", aşağıda belirtilen aday prompt isimlerinden biri OLMALIDIR VEYA giriş, aday promptlardan herhangi biri için uygun değilse \"DEFAULT\" olabilir. \\\n",
        "REMEMBER: \"next_inputs\", herhangi bir değişikliğin gerektiğini düşünmüyorsanız yalnızca orijinal girdi olabilir .\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (```json eklemeyi unutma)>>\"\"\""
      ],
      "metadata": {
        "id": "IIyP2Tb2CZwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "4AwCO_WvCbgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ],
      "metadata": {
        "id": "tGUYIOdDCdEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Kara cisim ışıması nedir?\") # Otomatik olarak fizik zincirine yönlendirilir\n",
        "chain.run(\"2 + 2 nedir?\") # Otomatik olarak matematik zincirine yönlendirilir\n",
        "chain.run(\"Neden vücudumuzdaki her hücre DNA içerir?\") # Otomatik olarak None seçilir ve dil modeline genel bir istek atılır"
      ],
      "metadata": {
        "id": "QqWzLir6CerM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}