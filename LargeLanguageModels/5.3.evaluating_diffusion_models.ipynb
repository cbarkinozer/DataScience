{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb Params\n",
    "MODEL_ARTIFACT = \"dlai-course/model-registry/SpriteGen:latest\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    # hyperparameters\n",
    "    num_samples = 30,\n",
    "    \n",
    "    # ddpm sampler hyperparameters\n",
    "    timesteps = 500,\n",
    "    beta1 = 1e-4,\n",
    "    beta2 = 0.02,\n",
    "    \n",
    "    # ddim sampler hp\n",
    "    ddim_n = 25,\n",
    "    \n",
    "    # network hyperparameters\n",
    "    height = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_artifact_name):\n",
    "    \"Load the model from wandb artifacts\"\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(model_artifact_name, type=\"model\")\n",
    "    model_path = Path(artifact.download())\n",
    "\n",
    "    # recover model info from the registry\n",
    "    producer_run = artifact.logged_by()\n",
    "\n",
    "    # load the weights dictionary\n",
    "    model_weights = torch.load(model_path/\"context_model.pth\", \n",
    "                               map_location=\"cpu\")\n",
    "\n",
    "    # create the model\n",
    "    model = ContextUnet(in_channels=3, \n",
    "                        n_feat=producer_run.config[\"n_feat\"], \n",
    "                        n_cfeat=producer_run.config[\"n_cfeat\"], \n",
    "                        height=producer_run.config[\"height\"])\n",
    "    \n",
    "    # load the weights into the model\n",
    "    model.load_state_dict(model_weights)\n",
    "\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = load_model(MODEL_ARTIFACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sample_ddpm_context = setup_ddpm(config.beta1, \n",
    "                                    config.beta2, \n",
    "                                    config.timesteps, \n",
    "                                    DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise vector\n",
    "# x_T ~ N(0, 1), sample initial noise\n",
    "noises = torch.randn(config.num_samples, 3, \n",
    "                     config.height, config.height).to(DEVICE)  \n",
    "\n",
    "# A fixed context vector to sample from\n",
    "ctx_vector = F.one_hot(torch.tensor([0,0,0,0,0,0,   # hero\n",
    "                                     1,1,1,1,1,1,   # non-hero\n",
    "                                     2,2,2,2,2,2,   # food\n",
    "                                     3,3,3,3,3,3,   # spell\n",
    "                                     4,4,4,4,4,4]), # side-facing \n",
    "                       5).to(DEVICE).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ddim_context = setup_ddim(config.beta1, \n",
    "                                 config.beta2, \n",
    "                                 config.timesteps, \n",
    "                                 DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm_samples, _ = sample_ddpm_context(nn_model, noises, ctx_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim_samples, _ = sample_ddim_context(nn_model, \n",
    "                                      noises, \n",
    "                                      ctx_vector, \n",
    "                                      n=config.ddim_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(columns=[\"input_noise\", \"ddpm\", \"ddim\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise, ddpm_s, ddim_s, c in zip(noises, \n",
    "                                    ddpm_samples, \n",
    "                                    ddim_samples, \n",
    "                                    to_classes(ctx_vector)):\n",
    "    \n",
    "    # add data row by row to the Table\n",
    "    table.add_data(wandb.Image(noise),\n",
    "                   wandb.Image(ddpm_s), \n",
    "                   wandb.Image(ddim_s),\n",
    "                   c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=\"dlai_sprite_diffusion\", \n",
    "                job_type=\"samplers_battle\", \n",
    "                config=config):\n",
    "    \n",
    "    wandb.log({\"samplers_table\":table})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
