{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUI3Vql68jAI"
   },
   "source": [
    "# Modeller, Promptlar ve Parser'lar\n",
    "Modeller dil modellerini yani LLM'leri kasteder. Prompt'lar dil modellerine verilen girdileri temsil eder. Parser'lar da alınacak çıktının formatını temsil eder.\n",
    "# Chat API: LangChain\n",
    "LangChain kullanarak OpenAI'ye API çağrısı atmayı deneyelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\cbark\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: openai in c:\\users\\cbark\\anaconda3\\lib\\site-packages (0.27.8)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.25.11)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Requirement already satisfied: langchain in c:\\users\\cbark\\anaconda3\\lib\\site-packages (0.0.198)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.7 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (0.0.9)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (1.10.9)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from langchain) (0.5.8)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.25.11)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (22.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\cbark\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cbark\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.25.11)\n"
     ]
    }
   ],
   "source": [
    "# Downloading and updating packages\n",
    "!pip install python-dotenv\n",
    "!pip install openai\n",
    "!pip install --upgrade langchain\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JLVDRCFC8tJG"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AU41QyuT8vS7"
   },
   "outputs": [],
   "source": [
    "# Loading config values\n",
    "    \n",
    "# Setting up the deployment name\n",
    "chatgpt_model_name = os.getenv('CHATGPT_MODEL')\n",
    "\n",
    "# API type is set to `azure`\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "\n",
    "# Currently Chat Completions API have the following versions available: 2023-03-15-preview\n",
    "openai.api_version = os.getenv('OPENAI_API_VERSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_message(messages, model_name, max_response_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=chatgpt_model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.5,\n",
    "        max_tokens=max_response_tokens,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Defining a function to print out the conversation in a readable format\n",
    "def print_conversation(messages):\n",
    "    for message in messages:\n",
    "        print(f\"[{message['role'].upper()}]\")\n",
    "        print(message['content'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant.\n"
     ]
    }
   ],
   "source": [
    "# System Message\n",
    "base_system_message = \"Sen yardımcı bir asistansın.\"\n",
    "\n",
    "system_message = f\"{base_system_message.strip()}\"\n",
    "print(system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbtvC66P8ycu"
   },
   "source": [
    "# Prompt Şablonu\n",
    "Prompt şablonu kullanıyoruz çünkü promptlar uzun ve detaylı olabiliyor bu nedenle mümkün olduğunda iyi promptları tekrar kullanmak istiyoruz. Longchain aynı zamanda popüler işlemler için promptlar sağlıyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-S08TMIX8xU5"
   },
   "outputs": [],
   "source": [
    "template_string = \"Üçlü ters tiklerle ayrılmış metni {style} olan bir \\\n",
    " stile çevirin. metin: ```{text}```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YtGClXtU892d"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xXlppRuN89lx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt # PrompTemplate()'e dair ayrıntılar\n",
    "\"\"\" \n",
    ">>> PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True)\n",
    "\"\"\"\n",
    "prompt_template.messages[0].prompt.input_variables # 2 çeşit girdi var \n",
    "# >>> ['style', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d5J4vX5N9FSj"
   },
   "outputs": [],
   "source": [
    "customer_style = \"Sakin ve saygılı bir tonda İstanbul Türkçesi.\"\n",
    "customer_email = \"Uiyy, blender kapağımın uçup mutfak duvarlarına smoothie sıçratmasına çok kızıyirum daa! Ve daha da kötüsü, garanti mutfağımı temizleme masrafını  karşılamıyor. Şu anda yardımına ihtiyacım var uşağum!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UENbDNQP9H_z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "--------------------------\n",
      "<class 'langchain.schema.HumanMessage'>\n",
      "--------------------------\n",
      "lc_kwargs={'content': 'Üçlü ters tiklerle ayrılmış metni Sakin ve saygılı bir tonda İstanbul Türkçesi. olan bir  stile çevirin. metin: ```Uiyy, blender kapağımın uçup mutfak duvarlarına smoothie sıçratmasına çok kızıyirum daa! Ve daha da kötüsü, garanti mutfağımı temizleme masrafını  karşılamıyor. Şu anda yardımına ihtiyacım var uşağum!```', 'additional_kwargs': {}} content='Üçlü ters tiklerle ayrılmış metni Sakin ve saygılı bir tonda İstanbul Türkçesi. olan bir  stile çevirin. metin: ```Uiyy, blender kapağımın uçup mutfak duvarlarına smoothie sıçratmasına çok kızıyirum daa! Ve daha da kötüsü, garanti mutfağımı temizleme masrafını  karşılamıyor. Şu anda yardımına ihtiyacım var uşağum!```' additional_kwargs={} example=False\n",
      "--------------------------\n",
      "Üçlü ters tiklerle ayrılmış metni Sakin ve saygılı bir tonda İstanbul Türkçesi. olan bir  stile çevirin. metin: ```Uiyy, blender kapağımın uçup mutfak duvarlarına smoothie sıçratmasına çok kızıyirum daa! Ve daha da kötüsü, garanti mutfağımı temizleme masrafını  karşılamıyor. Şu anda yardımına ihtiyacım var uşağum!```\n"
     ]
    }
   ],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)\n",
    "\n",
    "print(type(customer_messages)) \n",
    "print(\"--------------------------\")\n",
    "print(type(customer_messages[0])) # <class 'langchain.schema.HumanMessage'>\n",
    "print(\"--------------------------\")\n",
    "print(customer_messages[0])\n",
    "print(\"--------------------------\")\n",
    "print(customer_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 150\n"
     ]
    }
   ],
   "source": [
    "# Atılacak prompt budur değiştirebilirsiniz:\n",
    "user_message = customer_messages[0].content\n",
    "\n",
    "# Create the list of messages. role can be either \"user\" or \"assistant\" \n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"name\":\"example_user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "HeXtZskK9Kc_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM]\n",
      "You are a helpful assistant.\n",
      "\n",
      "[USER]\n",
      "Üçlü ters tiklerle ayrılmış metni Sakin ve saygılı bir tonda İstanbul Türkçesi. olan bir  stile çevirin. metin: ```Uiyy, blender kapağımın uçup mutfak duvarlarına smoothie sıçratmasına çok kızıyirum daa! Ve daha da kötüsü, garanti mutfağımı temizleme masrafını  karşılamıyor. Şu anda yardımına ihtiyacım var uşağum!```\n",
      "\n",
      "[ASSISTANT]\n",
      "\"Blender kapağımın uçarak mutfak duvarlarına smoothie sıçratmasından çok rahatsız oldum. Üstelik garanti kapsamında bile mutfak temizliği masrafını karşılamıyor. Şu anda yardımına ihtiyacım var, lütfen yardımcı olabilir misin?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Müşteri mesajının stiline çevirmek için LLM'yi çağırır\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "max_response_tokens = 500\n",
    "\n",
    "response = send_message(messages, chatgpt_model_name, max_response_tokens)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "print_conversation(messages) # Karadeniz ağızı ile yazılmış metinin İstanbul ağızı hali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2dohpy-B9MLl"
   },
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Merhaba müşterimiz, blenderi çalıştırmadan önce kapağını \\\n",
    "kapatmayı unutarak blenderinizi yanlış kullanmanız sizin hatanız olduğundan, \\\n",
    "garanti mutfağınızın temizlik masraflarını karşılamaz. Zor şans! Görüşürüz!\n",
    "\"\"\"\n",
    "service_style_karadeniz = \"\"\" Kibar tonda Karadeniz ağızıyla. Karadeniz ağızında uiy, daa, uşağum, haçan gibi kelimeler kullanır.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "                    style=service_style_karadeniz,\n",
    "                    text=service_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3Vw5JgF79OEN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 173\n"
     ]
    }
   ],
   "source": [
    "# Atılacak prompt budur değiştirebilirsiniz:\n",
    "user_message = service_messages[0].content\n",
    "\n",
    "# Create the list of messages. role can be either \"user\" or \"assistant\" \n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"name\":\"example_user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM]\n",
      "You are a helpful assistant.\n",
      "\n",
      "[USER]\n",
      "Üçlü ters tiklerle ayrılmış metni  Kibar tonda Karadeniz ağızıyla. Karadeniz ağızında uiy, daa, uşağum, haçan gibi kelimeler kullanır. olan bir  stile çevirin. metin: ```Merhaba müşterimiz, blenderi çalıştırmadan önce kapağını kapatmayı unutarak blenderinizi yanlış kullanmanız sizin hatanız olduğundan, garanti mutfağınızın temizlik masraflarını karşılamaz. Zor şans! Görüşürüz!\n",
      "```\n",
      "\n",
      "[ASSISTANT]\n",
      "\"Merhaba uşağum, blenderi çalıştırmadan önce haçan kapağını uiy kapatmayı unutarak blenderinizi yanlış kullanmanız sizin hatanız olduğundan, garanti mutfağınızın temizlik masraflarını karşılamaz. Zor şans daa! Görüşürüz!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Müşteri mesajının stiline çevirmek için LLM'yi çağırır\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "max_response_tokens = 500\n",
    "\n",
    "response = send_message(messages, chatgpt_model_name, max_response_tokens)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "print_conversation(messages) # İstanbul ağızı ile yazılmış cevabın Karadeniz ağızına çevirilmiş hali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMbYjbKn9XEy"
   },
   "source": [
    "# Çıktı Parser'ları\n",
    "LLM çıktısının nasıl görünmesini istediğimizi tanımlayarak başlayalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mZAts7BA9ZvF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'Oldukça uygun fiyatlı!'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"Oldukça uygun fiyatlı!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "5zmgwlWQ9cIJ"
   },
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "Bu yaprak üfleyici oldukça şaşırtıcı. Dört ayarı var: \\\n",
    "mum üfleyici, hafif esinti, rüzgarlı şehir ve kasırga. \\\n",
    "Eşimin yıldönümü hediyesi için tam zamanında, iki gün içinde geldi. \\\n",
    "Sanırım karım o kadar çok sevdi ki suskun kaldı. \\\n",
    "Şimdiye kadar onu kullanan tek kişi bendim ve iki günde bir çimenlerimizdeki yaprakları temizlemek için kullanıyorum. \\\n",
    "Diğer yaprak üfleyicilere göre biraz daha pahalıdır. \\\n",
    "var, ama ekstra özellikler için buna değer olduğunu düşünüyorum. \\\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "Aşağıdaki metin için aşağıdaki bilgileri çıkarın:\n",
    "\n",
    "hediye: Öğe başka biri için hediye olarak mı satın alındı? \\\n",
    "Cevabınız evet ise Doğru, değilse Yanlış veya bilinmiyor.\n",
    "\n",
    "teslimat_günleri: Ürün kaç günde vardı? \\\n",
    "Bu bilgi bulunamazsa -1 çıktısı verin.\n",
    "\n",
    "price_value: Değer veya fiyatla ilgili tüm cümleleri çıkarın,\\\n",
    "ve bunları virgülle ayrılmış bir Python listesi olarak çıktılayın.\n",
    "\n",
    "Çıktıyı aşağıdaki tuşlarla JSON olarak biçimlendirin(boolean değerler 'true' veya 'false' olmalıdır):\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "oAMqlr5z9dr5"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "messages = prompt_template.format_messages(text=customer_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 407\n"
     ]
    }
   ],
   "source": [
    "# Atılacak prompt budur değiştirebilirsiniz:\n",
    "user_message = messages[0].content\n",
    "\n",
    "# Create the list of messages. role can be either \"user\" or \"assistant\" \n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"name\":\"example_user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "6VhR3Mae9e5J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM]\n",
      "You are a helpful assistant.\n",
      "\n",
      "[USER]\n",
      "Aşağıdaki metin için aşağıdaki bilgileri çıkarın:\n",
      "\n",
      "hediye: Öğe başka biri için hediye olarak mı satın alındı? Cevabınız evet ise Doğru, değilse Yanlış veya bilinmiyor.\n",
      "\n",
      "teslimat_günleri: Ürün kaç günde vardı? Bu bilgi bulunamazsa -1 çıktısı verin.\n",
      "\n",
      "price_value: Değer veya fiyatla ilgili tüm cümleleri çıkarın,ve bunları virgülle ayrılmış bir Python listesi olarak çıktılayın.\n",
      "\n",
      "Çıktıyı aşağıdaki tuşlarla JSON olarak biçimlendirin(boolean değerler 'true' veya 'false' olmalıdır):\n",
      "gift\n",
      "delivery_days\n",
      "price_value\n",
      "\n",
      "text: Bu yaprak üfleyici oldukça şaşırtıcı. Dört ayarı var: mum üfleyici, hafif esinti, rüzgarlı şehir ve kasırga. Eşimin yıldönümü hediyesi için tam zamanında, iki gün içinde geldi. Sanırım karım o kadar çok sevdi ki suskun kaldı. Şimdiye kadar onu kullanan tek kişi bendim ve iki günde bir çimenlerimizdeki yaprakları temizlemek için kullanıyorum. Diğer yaprak üfleyicilere göre biraz daha pahalıdır. var, ama ekstra özellikler için buna değer olduğunu düşünüyorum. \n",
      "\n",
      "\n",
      "[ASSISTANT]\n",
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": [\"Diğer yaprak üfleyicilere göre biraz daha pahalıdır.\"]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n{\\n    \"gift\": true,\\n    \"delivery_days\": 2,\\n    \"price_value\": [\"Piyasadaki diğer yaprak üfleyicilerden biraz daha pahalı ama bence ekstra özellikler için buna değer.\"]\\n}\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_response_tokens = 500\n",
    "response = send_message(messages, chatgpt_model_name, max_response_tokens)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "print_conversation(messages)\n",
    "\"\"\"\n",
    "{\n",
    "    \"gift\": true,\n",
    "    \"delivery_days\": 2,\n",
    "    \"price_value\": [\"Piyasadaki diğer yaprak üfleyicilerden biraz daha pahalı ama bence ekstra özellikler için buna değer.\"]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WtsK0a99kZU"
   },
   "source": [
    "# LLM çıktı dizesini bir Python sözlüğüne ayrıştırın"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "leo5vr6J9m8f"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "Iy6AUQqh9plc"
   },
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Ürün başka biri için hediye olarak mı \\\n",
    " satın alındı? Cevabınız evet ise Doğru, değilse Yanlış veya bilinmiyor.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"Ürünün gelmesi kaç gün sürdü? \\\n",
    "Bu bilgi bulunamazsa -1 çıktısı alın.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Değer veya fiyatla ilgili tüm \\\n",
    " cümleleri çıkarın ve bunları virgülle ayrılmış bir Python listesi olarak çıkarın.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "rKhV_n9h9rbJ"
   },
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "SBkDI51l9sbh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Ürün başka biri için hediye olarak mı  satın alındı? Cevabınız evet ise Doğru, değilse Yanlış veya bilinmiyor.\n",
      "\t\"delivery_days\": string  // Ürünün gelmesi kaç gün sürdü? Bu bilgi bulunamazsa -1 çıktısı alın.\n",
      "\t\"price_value\": string  // Değer veya fiyatla ilgili tüm  cümleleri çıkarın ve bunları virgülle ayrılmış bir Python listesi olarak çıkarın.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "dslC6KhH9t7T"
   },
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\" Aşağıdaki metin için aşağıdaki bilgileri çıkarın:\n",
    "\n",
    "gift: Ürün başka biri için hediye olarak mı satın alındı? \\\n",
    "Cevabınız evet ise Doğru, değilse Yanlış veya bilinmiyor.\n",
    "\n",
    "delivery_days: Ürünün gelmesi kaç gün sürdü? Bu bilgi bulunamazsa -1 çıktısı \\\n",
    "alın.\n",
    "\n",
    "price_value: Değer veya fiyatla ilgili tüm cümleleri çıkarın ve \\\n",
    "bunları virgülle ayrılmış bir Python listesi olarak çıkarın.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 586\n"
     ]
    }
   ],
   "source": [
    "# Atılacak prompt budur değiştirebilirsiniz:\n",
    "user_message = messages[0].content\n",
    "\n",
    "# Create the list of messages. role can be either \"user\" or \"assistant\" \n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"name\":\"example_user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aşağıdaki metin için aşağıdaki bilgileri çıkarın:\n",
      "\n",
      "brand: Ürün markası nedir?\n",
      "\n",
      "rating: Ürünün derecelendirmesi kaç yıldızdır? Bu bilgi bulunamazsa -1 çıktısı alın.\n",
      "\n",
      "review_count: Ürün kaç yorum aldı? Bu bilgi bulunamazsa -1 çıktısı alın.\n",
      "\n",
      "text: Bu kahve makinesi gerçekten harika! Sadece bir düğmeye basarak favori kahvemi hazırlayabilirim. Marka olarak, bu ürünün kalitesine güveniyorum ve benim için mükemmel bir seçim oldu. Ürünün derecelendirmesi 5 üzerinden 4 yıldızdır ve 1000'den fazla yorum aldı. \n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"brand\": string  // Ürün markası nedir?\n",
      "\t\"rating\": string  // Ürünün derecelendirmesi kaç yıldızdır? Bu bilgi bulunamazsa -1 çıktısı alın.\n",
      "\t\"review_count\": string  // Ürün kaç yorum aldı? Bu bilgi bulunamazsa -1 çıktısı alın.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid return object. Expected key `gift` to be present, but got  and ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m send_message(messages, chatgpt_model_name, max_response_tokens)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m----> 6\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mtype\u001b[39m(output_dict) \u001b[38;5;66;03m# dict\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelivery_days\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\output_parsers\\structured.py:43\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     42\u001b[0m     expected_keys \u001b[38;5;241m=\u001b[39m [rs\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_schemas]\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\output_parsers\\json.py:37\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[1;34m(text, expected_keys)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_obj:\n\u001b[1;32m---> 37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid return object. Expected key `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be present, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m         )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json_obj\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Got invalid return object. Expected key `gift` to be present, but got  and "
     ]
    }
   ],
   "source": [
    "max_response_tokens = 500\n",
    "response = send_message(messages, chatgpt_model_name, max_response_tokens)\n",
    "\n",
    "print(response)\n",
    "\n",
    "output_dict = output_parser.parse(response)\n",
    "type(output_dict) # dict\n",
    "print(output_dict.get('delivery_days')) # '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
